{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11f6876814e8448a925acc35a49cc398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c6721184ab24177bcc0167606977e5d",
              "IPY_MODEL_cd6a309d6eac473889a08854e5561146",
              "IPY_MODEL_7734f238e22146ad87ba2c02bc7d7eab"
            ],
            "layout": "IPY_MODEL_940841a88e964351b628dfd63e21cc35"
          }
        },
        "7c6721184ab24177bcc0167606977e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af87c846731846cb8f12a7ed82b1c4b1",
            "placeholder": "​",
            "style": "IPY_MODEL_69c33ce7c0a44bf09eea20bb64f4b89a",
            "value": "100%"
          }
        },
        "cd6a309d6eac473889a08854e5561146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4329077db794c7b84e873b6e758bdb8",
            "max": 210,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_812916fe7d8f4b768a6303185b1faaa7",
            "value": 210
          }
        },
        "7734f238e22146ad87ba2c02bc7d7eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e802410d665640a8b1aef4029dd4c24c",
            "placeholder": "​",
            "style": "IPY_MODEL_98b1679f9a5b4a7aa2cd1a6b6e5486b1",
            "value": " 210/210 [01:04&lt;00:00,  3.48it/s]"
          }
        },
        "940841a88e964351b628dfd63e21cc35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af87c846731846cb8f12a7ed82b1c4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c33ce7c0a44bf09eea20bb64f4b89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4329077db794c7b84e873b6e758bdb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812916fe7d8f4b768a6303185b1faaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e802410d665640a8b1aef4029dd4c24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b1679f9a5b4a7aa2cd1a6b6e5486b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "nno3b0AsiFcD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk8BJD8ViblH",
        "outputId": "ebe73fd7-1e84-41b2-b3ec-1e9e2f956b19"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/AIP/A2/PascalVOC.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXB5h-_FieWh",
        "outputId": "add64c9e-0f82-466e-c8a6-b2ed66275fd1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/AIP/A2/PascalVOC.zip\n",
            "replace PascalVOC/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "c_NLUqE8h7ix"
      },
      "outputs": [],
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Define a new classifier\n",
        "num_classes = 21\n",
        "\n",
        "# Encoder\n",
        "features = nn.Sequential(*list(model.children())[:-1])                # Extract layers up to and excluding the classifier\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder = nn.Sequential(\n",
        "    nn.ConvTranspose2d(1280, 320, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "    nn.BatchNorm2d(320),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(320, 96, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "    nn.BatchNorm2d(96),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(96, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(16, 21, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Combining both:\n",
        "########################################\n",
        "\n",
        "model = nn.Sequential(features, decoder)\n",
        "\n",
        "\n",
        "########################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ch3_to_ch1(img):\n",
        "\n",
        "  num_color = np.unique(img.reshape(-1, img.shape[2]), axis = 0)\n",
        "\n",
        "  generated_img = np.zeros(img.shape[:2]).astype(np.uint8)\n",
        "\n",
        "  for col in num_color:\n",
        "    \n",
        "    locs = np.where(np.all(img==col,axis=2))\n",
        "\n",
        "    try:\n",
        "      generated_img[locs[0], locs[1]] = np.where(np.all(colors == col, axis=1))[0][0]\n",
        "      \n",
        "    except Exception as e:\n",
        "      pass\n",
        "  return generated_img\n"
      ],
      "metadata": {
        "id": "zlv5q2VPiSc6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(mask_dir)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.img_dir, self.images[index].replace(\".png\", \".jpg\"))\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".png\", \".png\"))\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
        "    mask[mask == 255.0] = 1.0\n",
        "    \n",
        "    if self.transform is not None:\n",
        "        # transform_img = transforms.Compose([self.transform, transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "        # transform_label = transforms.Compose([self.transform])\n",
        "\n",
        "        transform_img = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Resize((224, 224)),\n",
        "                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "        image = transform_img(image)\n",
        "        \n",
        "        mask = ch3_to_ch1(mask)\n",
        "        transform_mask = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Resize((224, 224))])\n",
        "        mask = transform_mask(mask)\n",
        "\n",
        "    return image, mask\n",
        "\n"
      ],
      "metadata": {
        "id": "5o_fh3hBiC50"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformations\n",
        "\n",
        "transform_image = transforms.Compose([ transforms.ToTensor(),\n",
        "                                transforms.Resize((224, 224))])\n",
        "\n",
        "\n",
        "# create the train and test datasets\n",
        "\n",
        "trainDS = ImgDataset(img_dir=\"/content/PascalVOC/trainval/Images\", mask_dir=\"/content/PascalVOC/trainval/Annotations\",\n",
        "\ttransform=transform_image)\n",
        "testDS = ImgDataset(img_dir=\"/content/PascalVOC/test/Images\", mask_dir=\"/content/PascalVOC/test/Annotations\",\n",
        "    transform=transform_image)\n",
        "\n",
        "print(f\"found {len(trainDS)} examples in the training set...\")\n",
        "print(f\"found {len(testDS)} examples in the test set...\")\n",
        "\n",
        "# create the training and test data loaders\n",
        "trainLoader = DataLoader(trainDS, shuffle=True, batch_size = 8)\n",
        "testLoader = DataLoader(testDS, shuffle=False,\tbatch_size = 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFG8NRFiiJpW",
        "outputId": "d363bc57-34d3-468b-e1ff-a3e26d0d7265"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 422 examples in the training set...\n",
            "found 210 examples in the test set...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainLoader)\n",
        "trainLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2oo1iSqiMPO",
        "outputId": "bc587224-ac77-4926-f334-c360912dde78"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7ff922e7aca0>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_MobileNetV2(model, trainLoader):\n",
        "  epochs = 25\n",
        "  params_to_update = []\n",
        "  for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      params_to_update.append(param)\n",
        "      # print(\"Parameters to Update by optimizer:\",name)\n",
        "      \n",
        "  optimizer = optim.Adam(params_to_update, lr=0.001)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "  model = model.to(\"cuda\")\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.00\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, masks in trainLoader:\n",
        "      inputs = inputs.to(\"cuda\")\n",
        "      masks = masks.to(\"cuda\")\n",
        "\n",
        "      optimizer.zero_grad()   # zero the parameter gradients at the start of training loop\n",
        "      \n",
        "      outputs = model(inputs)\n",
        "\n",
        "      masks = masks.clone().squeeze().detach().long()\n",
        "      outputs = outputs.type(torch.float64)\n",
        "\n",
        "      loss = criterion(outputs, masks)\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "      print(f\"training loss: {loss.item():.5f}\")\n",
        "\n",
        "  return model\n",
        "\n",
        "modelV2 = train_MobileNetV2(model, trainLoader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z90KSGpiP3S",
        "outputId": "cdf4b3af-85ef-4e34-8d89-907c1b15ecc3"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 3.04173\n",
            "training loss: 3.03322\n",
            "training loss: 3.02451\n",
            "training loss: 3.01626\n",
            "training loss: 3.00852\n",
            "training loss: 3.00096\n",
            "training loss: 2.99340\n",
            "training loss: 2.98600\n",
            "training loss: 2.97863\n",
            "training loss: 2.97111\n",
            "training loss: 2.96344\n",
            "training loss: 2.95589\n",
            "training loss: 2.94809\n",
            "training loss: 2.94025\n",
            "training loss: 2.93251\n",
            "training loss: 2.92440\n",
            "training loss: 2.91633\n",
            "training loss: 2.90820\n",
            "training loss: 2.89992\n",
            "training loss: 2.89155\n",
            "training loss: 2.88315\n",
            "training loss: 2.87476\n",
            "training loss: 2.86628\n",
            "training loss: 2.85781\n",
            "training loss: 2.84930\n",
            "training loss: 2.84074\n",
            "training loss: 2.83225\n",
            "training loss: 2.82373\n",
            "training loss: 2.81525\n",
            "training loss: 2.80680\n",
            "training loss: 2.79836\n",
            "training loss: 2.78998\n",
            "training loss: 2.78159\n",
            "training loss: 2.77333\n",
            "training loss: 2.76511\n",
            "training loss: 2.75689\n",
            "training loss: 2.74876\n",
            "training loss: 2.74073\n",
            "training loss: 2.73268\n",
            "training loss: 2.72473\n",
            "training loss: 2.71689\n",
            "training loss: 2.70909\n",
            "training loss: 2.70134\n",
            "training loss: 2.69367\n",
            "training loss: 2.68609\n",
            "training loss: 2.67862\n",
            "training loss: 2.67117\n",
            "training loss: 2.66383\n",
            "training loss: 2.65659\n",
            "training loss: 2.64944\n",
            "training loss: 2.64238\n",
            "training loss: 2.63540\n",
            "training loss: 2.62851\n",
            "training loss: 2.62171\n",
            "training loss: 2.61496\n",
            "training loss: 2.60832\n",
            "training loss: 2.60177\n",
            "training loss: 2.59530\n",
            "training loss: 2.58891\n",
            "training loss: 2.58261\n",
            "training loss: 2.57639\n",
            "training loss: 2.57026\n",
            "training loss: 2.56420\n",
            "training loss: 2.55823\n",
            "training loss: 2.55236\n",
            "training loss: 2.54655\n",
            "training loss: 2.54083\n",
            "training loss: 2.53519\n",
            "training loss: 2.52962\n",
            "training loss: 2.52414\n",
            "training loss: 2.51874\n",
            "training loss: 2.51341\n",
            "training loss: 2.50816\n",
            "training loss: 2.50298\n",
            "training loss: 2.49788\n",
            "training loss: 2.49285\n",
            "training loss: 2.48789\n",
            "training loss: 2.48301\n",
            "training loss: 2.47820\n",
            "training loss: 2.47347\n",
            "training loss: 2.46880\n",
            "training loss: 2.46420\n",
            "training loss: 2.45967\n",
            "training loss: 2.45521\n",
            "training loss: 2.45082\n",
            "training loss: 2.44648\n",
            "training loss: 2.44222\n",
            "training loss: 2.43802\n",
            "training loss: 2.43388\n",
            "training loss: 2.42980\n",
            "training loss: 2.42579\n",
            "training loss: 2.42183\n",
            "training loss: 2.41794\n",
            "training loss: 2.41411\n",
            "training loss: 2.41033\n",
            "training loss: 2.40661\n",
            "training loss: 2.40295\n",
            "training loss: 2.39935\n",
            "training loss: 2.39579\n",
            "training loss: 2.39230\n",
            "training loss: 2.38885\n",
            "training loss: 2.38546\n",
            "training loss: 2.38212\n",
            "training loss: 2.37883\n",
            "training loss: 2.37559\n",
            "training loss: 2.37240\n",
            "training loss: 2.36926\n",
            "training loss: 2.36616\n",
            "training loss: 2.36312\n",
            "training loss: 2.36012\n",
            "training loss: 2.35717\n",
            "training loss: 2.35426\n",
            "training loss: 2.35140\n",
            "training loss: 2.34858\n",
            "training loss: 2.34580\n",
            "training loss: 2.34306\n",
            "training loss: 2.34037\n",
            "training loss: 2.33772\n",
            "training loss: 2.33511\n",
            "training loss: 2.33253\n",
            "training loss: 2.33000\n",
            "training loss: 2.32751\n",
            "training loss: 2.32505\n",
            "training loss: 2.32263\n",
            "training loss: 2.32025\n",
            "training loss: 2.31790\n",
            "training loss: 2.31559\n",
            "training loss: 2.31331\n",
            "training loss: 2.31107\n",
            "training loss: 2.30886\n",
            "training loss: 2.30668\n",
            "training loss: 2.30454\n",
            "training loss: 2.30242\n",
            "training loss: 2.30034\n",
            "training loss: 2.29829\n",
            "training loss: 2.29627\n",
            "training loss: 2.29428\n",
            "training loss: 2.29232\n",
            "training loss: 2.29039\n",
            "training loss: 2.28848\n",
            "training loss: 2.28661\n",
            "training loss: 2.28476\n",
            "training loss: 2.28294\n",
            "training loss: 2.28114\n",
            "training loss: 2.27937\n",
            "training loss: 2.27763\n",
            "training loss: 2.27591\n",
            "training loss: 2.27422\n",
            "training loss: 2.27255\n",
            "training loss: 2.27091\n",
            "training loss: 2.26929\n",
            "training loss: 2.26769\n",
            "training loss: 2.26611\n",
            "training loss: 2.26456\n",
            "training loss: 2.26303\n",
            "training loss: 2.26152\n",
            "training loss: 2.26003\n",
            "training loss: 2.25857\n",
            "training loss: 2.25712\n",
            "training loss: 2.25570\n",
            "training loss: 2.25429\n",
            "training loss: 2.25291\n",
            "training loss: 2.25154\n",
            "training loss: 2.25019\n",
            "training loss: 2.24886\n",
            "training loss: 2.24755\n",
            "training loss: 2.24626\n",
            "training loss: 2.24499\n",
            "training loss: 2.24373\n",
            "training loss: 2.24249\n",
            "training loss: 2.24127\n",
            "training loss: 2.24006\n",
            "training loss: 2.23887\n",
            "training loss: 2.23770\n",
            "training loss: 2.23654\n",
            "training loss: 2.23540\n",
            "training loss: 2.23427\n",
            "training loss: 2.23316\n",
            "training loss: 2.23207\n",
            "training loss: 2.23098\n",
            "training loss: 2.22992\n",
            "training loss: 2.22886\n",
            "training loss: 2.22782\n",
            "training loss: 2.22680\n",
            "training loss: 2.22579\n",
            "training loss: 2.22479\n",
            "training loss: 2.22380\n",
            "training loss: 2.22283\n",
            "training loss: 2.22187\n",
            "training loss: 2.22092\n",
            "training loss: 2.21999\n",
            "training loss: 2.21907\n",
            "training loss: 2.21816\n",
            "training loss: 2.21726\n",
            "training loss: 2.21637\n",
            "training loss: 2.21549\n",
            "training loss: 2.21463\n",
            "training loss: 2.21377\n",
            "training loss: 2.21293\n",
            "training loss: 2.21210\n",
            "training loss: 2.21128\n",
            "training loss: 2.21047\n",
            "training loss: 2.20967\n",
            "training loss: 2.20888\n",
            "training loss: 2.20809\n",
            "training loss: 2.20732\n",
            "training loss: 2.20656\n",
            "training loss: 2.20581\n",
            "training loss: 2.20507\n",
            "training loss: 2.20433\n",
            "training loss: 2.20361\n",
            "training loss: 2.20289\n",
            "training loss: 2.20219\n",
            "training loss: 2.20149\n",
            "training loss: 2.20080\n",
            "training loss: 2.20012\n",
            "training loss: 2.19945\n",
            "training loss: 2.19878\n",
            "training loss: 2.19813\n",
            "training loss: 2.19748\n",
            "training loss: 2.19684\n",
            "training loss: 2.19620\n",
            "training loss: 2.19558\n",
            "training loss: 2.19496\n",
            "training loss: 2.19435\n",
            "training loss: 2.19374\n",
            "training loss: 2.19315\n",
            "training loss: 2.19256\n",
            "training loss: 2.19198\n",
            "training loss: 2.19140\n",
            "training loss: 2.19083\n",
            "training loss: 2.19027\n",
            "training loss: 2.18971\n",
            "training loss: 2.18916\n",
            "training loss: 2.18862\n",
            "training loss: 2.18808\n",
            "training loss: 2.18755\n",
            "training loss: 2.18703\n",
            "training loss: 2.18651\n",
            "training loss: 2.18600\n",
            "training loss: 2.18549\n",
            "training loss: 2.18499\n",
            "training loss: 2.18450\n",
            "training loss: 2.18401\n",
            "training loss: 2.18352\n",
            "training loss: 2.18304\n",
            "training loss: 2.18257\n",
            "training loss: 2.18210\n",
            "training loss: 2.18164\n",
            "training loss: 2.18118\n",
            "training loss: 2.18073\n",
            "training loss: 2.18028\n",
            "training loss: 2.17984\n",
            "training loss: 2.17940\n",
            "training loss: 2.17897\n",
            "training loss: 2.17854\n",
            "training loss: 2.17812\n",
            "training loss: 2.17770\n",
            "training loss: 2.17728\n",
            "training loss: 2.17687\n",
            "training loss: 2.17647\n",
            "training loss: 2.17607\n",
            "training loss: 2.17567\n",
            "training loss: 2.17528\n",
            "training loss: 2.17489\n",
            "training loss: 2.17451\n",
            "training loss: 2.17413\n",
            "training loss: 2.17375\n",
            "training loss: 2.17338\n",
            "training loss: 2.17301\n",
            "training loss: 2.17264\n",
            "training loss: 2.17228\n",
            "training loss: 2.17193\n",
            "training loss: 2.17157\n",
            "training loss: 2.17123\n",
            "training loss: 2.17088\n",
            "training loss: 2.17054\n",
            "training loss: 2.17020\n",
            "training loss: 2.16986\n",
            "training loss: 2.16953\n",
            "training loss: 2.16920\n",
            "training loss: 2.16888\n",
            "training loss: 2.16856\n",
            "training loss: 2.16824\n",
            "training loss: 2.16792\n",
            "training loss: 2.16761\n",
            "training loss: 2.16730\n",
            "training loss: 2.16700\n",
            "training loss: 2.16669\n",
            "training loss: 2.16639\n",
            "training loss: 2.16610\n",
            "training loss: 2.16580\n",
            "training loss: 2.16551\n",
            "training loss: 2.16522\n",
            "training loss: 2.16494\n",
            "training loss: 2.16466\n",
            "training loss: 2.16438\n",
            "training loss: 2.16410\n",
            "training loss: 2.16383\n",
            "training loss: 2.16355\n",
            "training loss: 2.16329\n",
            "training loss: 2.16302\n",
            "training loss: 2.16276\n",
            "training loss: 2.16249\n",
            "training loss: 2.16224\n",
            "training loss: 2.16198\n",
            "training loss: 2.16173\n",
            "training loss: 2.16147\n",
            "training loss: 2.16122\n",
            "training loss: 2.16098\n",
            "training loss: 2.16073\n",
            "training loss: 2.16049\n",
            "training loss: 2.16025\n",
            "training loss: 2.16001\n",
            "training loss: 2.15978\n",
            "training loss: 2.15955\n",
            "training loss: 2.15932\n",
            "training loss: 2.15909\n",
            "training loss: 2.15886\n",
            "training loss: 2.15863\n",
            "training loss: 2.15841\n",
            "training loss: 2.15819\n",
            "training loss: 2.15797\n",
            "training loss: 2.15776\n",
            "training loss: 2.15754\n",
            "training loss: 2.15733\n",
            "training loss: 2.15712\n",
            "training loss: 2.15691\n",
            "training loss: 2.15670\n",
            "training loss: 2.15650\n",
            "training loss: 2.15629\n",
            "training loss: 2.15609\n",
            "training loss: 2.15589\n",
            "training loss: 2.15570\n",
            "training loss: 2.15550\n",
            "training loss: 2.15531\n",
            "training loss: 2.15511\n",
            "training loss: 2.15492\n",
            "training loss: 2.15473\n",
            "training loss: 2.15454\n",
            "training loss: 2.15436\n",
            "training loss: 2.15417\n",
            "training loss: 2.15399\n",
            "training loss: 2.15381\n",
            "training loss: 2.15363\n",
            "training loss: 2.15345\n",
            "training loss: 2.15328\n",
            "training loss: 2.15310\n",
            "training loss: 2.15293\n",
            "training loss: 2.15275\n",
            "training loss: 2.15258\n",
            "training loss: 2.15241\n",
            "training loss: 2.15225\n",
            "training loss: 2.15208\n",
            "training loss: 2.15192\n",
            "training loss: 2.15175\n",
            "training loss: 2.15159\n",
            "training loss: 2.15143\n",
            "training loss: 2.15127\n",
            "training loss: 2.15111\n",
            "training loss: 2.15095\n",
            "training loss: 2.15080\n",
            "training loss: 2.15064\n",
            "training loss: 2.15049\n",
            "training loss: 2.15034\n",
            "training loss: 2.15019\n",
            "training loss: 2.15004\n",
            "training loss: 2.14989\n",
            "training loss: 2.14974\n",
            "training loss: 2.14960\n",
            "training loss: 2.14945\n",
            "training loss: 2.14931\n",
            "training loss: 2.14917\n",
            "training loss: 2.14903\n",
            "training loss: 2.14889\n",
            "training loss: 2.14875\n",
            "training loss: 2.14861\n",
            "training loss: 2.14847\n",
            "training loss: 2.14834\n",
            "training loss: 2.14820\n",
            "training loss: 2.14807\n",
            "training loss: 2.14794\n",
            "training loss: 2.14781\n",
            "training loss: 2.14768\n",
            "training loss: 2.14755\n",
            "training loss: 2.14742\n",
            "training loss: 2.14729\n",
            "training loss: 2.14717\n",
            "training loss: 2.14704\n",
            "training loss: 2.14692\n",
            "training loss: 2.14679\n",
            "training loss: 2.14667\n",
            "training loss: 2.14655\n",
            "training loss: 2.14643\n",
            "training loss: 2.14631\n",
            "training loss: 2.14619\n",
            "training loss: 2.14607\n",
            "training loss: 2.14595\n",
            "training loss: 2.14584\n",
            "training loss: 2.14572\n",
            "training loss: 2.14561\n",
            "training loss: 2.14549\n",
            "training loss: 2.14538\n",
            "training loss: 2.14527\n",
            "training loss: 2.14516\n",
            "training loss: 2.14505\n",
            "training loss: 2.14494\n",
            "training loss: 2.14483\n",
            "training loss: 2.14472\n",
            "training loss: 2.14462\n",
            "training loss: 2.14451\n",
            "training loss: 2.14441\n",
            "training loss: 2.14430\n",
            "training loss: 2.14420\n",
            "training loss: 2.14409\n",
            "training loss: 2.14399\n",
            "training loss: 2.14389\n",
            "training loss: 2.14379\n",
            "training loss: 2.14369\n",
            "training loss: 2.14359\n",
            "training loss: 2.14349\n",
            "training loss: 2.14339\n",
            "training loss: 2.14330\n",
            "training loss: 2.14320\n",
            "training loss: 2.14310\n",
            "training loss: 2.14301\n",
            "training loss: 2.14291\n",
            "training loss: 2.14282\n",
            "training loss: 2.14273\n",
            "training loss: 2.14263\n",
            "training loss: 2.14254\n",
            "training loss: 2.14245\n",
            "training loss: 2.14236\n",
            "training loss: 2.14227\n",
            "training loss: 2.14218\n",
            "training loss: 2.14209\n",
            "training loss: 2.14200\n",
            "training loss: 2.14192\n",
            "training loss: 2.14183\n",
            "training loss: 2.14174\n",
            "training loss: 2.14166\n",
            "training loss: 2.14157\n",
            "training loss: 2.14149\n",
            "training loss: 2.14140\n",
            "training loss: 2.14132\n",
            "training loss: 2.14124\n",
            "training loss: 2.14116\n",
            "training loss: 2.14107\n",
            "training loss: 2.14099\n",
            "training loss: 2.14091\n",
            "training loss: 2.14083\n",
            "training loss: 2.14075\n",
            "training loss: 2.14067\n",
            "training loss: 2.14059\n",
            "training loss: 2.14052\n",
            "training loss: 2.14044\n",
            "training loss: 2.14036\n",
            "training loss: 2.14029\n",
            "training loss: 2.14021\n",
            "training loss: 2.14013\n",
            "training loss: 2.14006\n",
            "training loss: 2.13999\n",
            "training loss: 2.13991\n",
            "training loss: 2.13984\n",
            "training loss: 2.13977\n",
            "training loss: 2.13969\n",
            "training loss: 2.13962\n",
            "training loss: 2.13955\n",
            "training loss: 2.13948\n",
            "training loss: 2.13941\n",
            "training loss: 2.13934\n",
            "training loss: 2.13927\n",
            "training loss: 2.13920\n",
            "training loss: 2.13913\n",
            "training loss: 2.13906\n",
            "training loss: 2.13899\n",
            "training loss: 2.13893\n",
            "training loss: 2.13886\n",
            "training loss: 2.13879\n",
            "training loss: 2.13873\n",
            "training loss: 2.13866\n",
            "training loss: 2.13859\n",
            "training loss: 2.13853\n",
            "training loss: 2.13846\n",
            "training loss: 2.13840\n",
            "training loss: 2.13834\n",
            "training loss: 2.13827\n",
            "training loss: 2.13821\n",
            "training loss: 2.13815\n",
            "training loss: 2.13809\n",
            "training loss: 2.13802\n",
            "training loss: 2.13796\n",
            "training loss: 2.13790\n",
            "training loss: 2.13784\n",
            "training loss: 2.13778\n",
            "training loss: 2.13772\n",
            "training loss: 2.13766\n",
            "training loss: 2.13760\n",
            "training loss: 2.13754\n",
            "training loss: 2.13748\n",
            "training loss: 2.13742\n",
            "training loss: 2.13737\n",
            "training loss: 2.13731\n",
            "training loss: 2.13725\n",
            "training loss: 2.13720\n",
            "training loss: 2.13714\n",
            "training loss: 2.13708\n",
            "training loss: 2.13703\n",
            "training loss: 2.13697\n",
            "training loss: 2.13692\n",
            "training loss: 2.13686\n",
            "training loss: 2.13681\n",
            "training loss: 2.13675\n",
            "training loss: 2.13670\n",
            "training loss: 2.13665\n",
            "training loss: 2.13659\n",
            "training loss: 2.13654\n",
            "training loss: 2.13649\n",
            "training loss: 2.13644\n",
            "training loss: 2.13638\n",
            "training loss: 2.13633\n",
            "training loss: 2.13628\n",
            "training loss: 2.13623\n",
            "training loss: 2.13618\n",
            "training loss: 2.13613\n",
            "training loss: 2.13608\n",
            "training loss: 2.13603\n",
            "training loss: 2.13598\n",
            "training loss: 2.13593\n",
            "training loss: 2.13588\n",
            "training loss: 2.13583\n",
            "training loss: 2.13578\n",
            "training loss: 2.13573\n",
            "training loss: 2.13569\n",
            "training loss: 2.13564\n",
            "training loss: 2.13559\n",
            "training loss: 2.13554\n",
            "training loss: 2.13550\n",
            "training loss: 2.13545\n",
            "training loss: 2.13541\n",
            "training loss: 2.13536\n",
            "training loss: 2.13531\n",
            "training loss: 2.13527\n",
            "training loss: 2.13522\n",
            "training loss: 2.13518\n",
            "training loss: 2.13513\n",
            "training loss: 2.13509\n",
            "training loss: 2.13504\n",
            "training loss: 2.13500\n",
            "training loss: 2.13496\n",
            "training loss: 2.13491\n",
            "training loss: 2.13487\n",
            "training loss: 2.13483\n",
            "training loss: 2.13478\n",
            "training loss: 2.13474\n",
            "training loss: 2.13470\n",
            "training loss: 2.13466\n",
            "training loss: 2.13461\n",
            "training loss: 2.13457\n",
            "training loss: 2.13453\n",
            "training loss: 2.13449\n",
            "training loss: 2.13445\n",
            "training loss: 2.13441\n",
            "training loss: 2.13437\n",
            "training loss: 2.13433\n",
            "training loss: 2.13429\n",
            "training loss: 2.13425\n",
            "training loss: 2.13421\n",
            "training loss: 2.13417\n",
            "training loss: 2.13413\n",
            "training loss: 2.13409\n",
            "training loss: 2.13405\n",
            "training loss: 2.13401\n",
            "training loss: 2.13397\n",
            "training loss: 2.13394\n",
            "training loss: 2.13390\n",
            "training loss: 2.13386\n",
            "training loss: 2.13382\n",
            "training loss: 2.13378\n",
            "training loss: 2.13375\n",
            "training loss: 2.13371\n",
            "training loss: 2.13367\n",
            "training loss: 2.13364\n",
            "training loss: 2.13360\n",
            "training loss: 2.13356\n",
            "training loss: 2.13353\n",
            "training loss: 2.13349\n",
            "training loss: 2.13346\n",
            "training loss: 2.13342\n",
            "training loss: 2.13339\n",
            "training loss: 2.13335\n",
            "training loss: 2.13332\n",
            "training loss: 2.13328\n",
            "training loss: 2.13325\n",
            "training loss: 2.13321\n",
            "training loss: 2.13318\n",
            "training loss: 2.13314\n",
            "training loss: 2.13311\n",
            "training loss: 2.13308\n",
            "training loss: 2.13304\n",
            "training loss: 2.13301\n",
            "training loss: 2.13298\n",
            "training loss: 2.13294\n",
            "training loss: 2.13291\n",
            "training loss: 2.13288\n",
            "training loss: 2.13284\n",
            "training loss: 2.13281\n",
            "training loss: 2.13278\n",
            "training loss: 2.13275\n",
            "training loss: 2.13272\n",
            "training loss: 2.13268\n",
            "training loss: 2.13265\n",
            "training loss: 2.13262\n",
            "training loss: 2.13259\n",
            "training loss: 2.13256\n",
            "training loss: 2.13253\n",
            "training loss: 2.13250\n",
            "training loss: 2.13247\n",
            "training loss: 2.13243\n",
            "training loss: 2.13240\n",
            "training loss: 2.13237\n",
            "training loss: 2.13234\n",
            "training loss: 2.13231\n",
            "training loss: 2.13228\n",
            "training loss: 2.13225\n",
            "training loss: 2.13223\n",
            "training loss: 2.13220\n",
            "training loss: 2.13217\n",
            "training loss: 2.13214\n",
            "training loss: 2.13211\n",
            "training loss: 2.13208\n",
            "training loss: 2.13205\n",
            "training loss: 2.13202\n",
            "training loss: 2.13199\n",
            "training loss: 2.13197\n",
            "training loss: 2.13194\n",
            "training loss: 2.13191\n",
            "training loss: 2.13188\n",
            "training loss: 2.13185\n",
            "training loss: 2.13183\n",
            "training loss: 2.13180\n",
            "training loss: 2.13177\n",
            "training loss: 2.13174\n",
            "training loss: 2.13172\n",
            "training loss: 2.13169\n",
            "training loss: 2.13166\n",
            "training loss: 2.13164\n",
            "training loss: 2.13161\n",
            "training loss: 2.13158\n",
            "training loss: 2.13156\n",
            "training loss: 2.13153\n",
            "training loss: 2.13150\n",
            "training loss: 2.13148\n",
            "training loss: 2.13145\n",
            "training loss: 2.13143\n",
            "training loss: 2.13140\n",
            "training loss: 2.13138\n",
            "training loss: 2.13135\n",
            "training loss: 2.13133\n",
            "training loss: 2.13130\n",
            "training loss: 2.13128\n",
            "training loss: 2.13125\n",
            "training loss: 2.13123\n",
            "training loss: 2.13120\n",
            "training loss: 2.13118\n",
            "training loss: 2.13115\n",
            "training loss: 2.13113\n",
            "training loss: 2.13110\n",
            "training loss: 2.13108\n",
            "training loss: 2.13106\n",
            "training loss: 2.13103\n",
            "training loss: 2.13101\n",
            "training loss: 2.13098\n",
            "training loss: 2.13096\n",
            "training loss: 2.13094\n",
            "training loss: 2.13091\n",
            "training loss: 2.13089\n",
            "training loss: 2.13087\n",
            "training loss: 2.13084\n",
            "training loss: 2.13082\n",
            "training loss: 2.13080\n",
            "training loss: 2.13078\n",
            "training loss: 2.13075\n",
            "training loss: 2.13073\n",
            "training loss: 2.13071\n",
            "training loss: 2.13069\n",
            "training loss: 2.13066\n",
            "training loss: 2.13064\n",
            "training loss: 2.13062\n",
            "training loss: 2.13060\n",
            "training loss: 2.13057\n",
            "training loss: 2.13055\n",
            "training loss: 2.13053\n",
            "training loss: 2.13051\n",
            "training loss: 2.13049\n",
            "training loss: 2.13047\n",
            "training loss: 2.13045\n",
            "training loss: 2.13042\n",
            "training loss: 2.13040\n",
            "training loss: 2.13038\n",
            "training loss: 2.13036\n",
            "training loss: 2.13034\n",
            "training loss: 2.13032\n",
            "training loss: 2.13030\n",
            "training loss: 2.13028\n",
            "training loss: 2.13026\n",
            "training loss: 2.13024\n",
            "training loss: 2.13022\n",
            "training loss: 2.13020\n",
            "training loss: 2.13018\n",
            "training loss: 2.13016\n",
            "training loss: 2.13014\n",
            "training loss: 2.13012\n",
            "training loss: 2.13010\n",
            "training loss: 2.13008\n",
            "training loss: 2.13006\n",
            "training loss: 2.13004\n",
            "training loss: 2.13002\n",
            "training loss: 2.13000\n",
            "training loss: 2.12998\n",
            "training loss: 2.12996\n",
            "training loss: 2.12994\n",
            "training loss: 2.12992\n",
            "training loss: 2.12990\n",
            "training loss: 2.12988\n",
            "training loss: 2.12987\n",
            "training loss: 2.12985\n",
            "training loss: 2.12983\n",
            "training loss: 2.12981\n",
            "training loss: 2.12979\n",
            "training loss: 2.12977\n",
            "training loss: 2.12975\n",
            "training loss: 2.12974\n",
            "training loss: 2.12972\n",
            "training loss: 2.12970\n",
            "training loss: 2.12968\n",
            "training loss: 2.12966\n",
            "training loss: 2.12965\n",
            "training loss: 2.12963\n",
            "training loss: 2.12961\n",
            "training loss: 2.12959\n",
            "training loss: 2.12957\n",
            "training loss: 2.12956\n",
            "training loss: 2.12954\n",
            "training loss: 2.12952\n",
            "training loss: 2.12951\n",
            "training loss: 2.12949\n",
            "training loss: 2.12947\n",
            "training loss: 2.12945\n",
            "training loss: 2.12944\n",
            "training loss: 2.12942\n",
            "training loss: 2.12940\n",
            "training loss: 2.12939\n",
            "training loss: 2.12937\n",
            "training loss: 2.12935\n",
            "training loss: 2.12934\n",
            "training loss: 2.12932\n",
            "training loss: 2.12930\n",
            "training loss: 2.12929\n",
            "training loss: 2.12927\n",
            "training loss: 2.12925\n",
            "training loss: 2.12924\n",
            "training loss: 2.12922\n",
            "training loss: 2.12920\n",
            "training loss: 2.12919\n",
            "training loss: 2.12917\n",
            "training loss: 2.12916\n",
            "training loss: 2.12914\n",
            "training loss: 2.12912\n",
            "training loss: 2.12911\n",
            "training loss: 2.12909\n",
            "training loss: 2.12908\n",
            "training loss: 2.12906\n",
            "training loss: 2.12905\n",
            "training loss: 2.12903\n",
            "training loss: 2.12902\n",
            "training loss: 2.12900\n",
            "training loss: 2.12899\n",
            "training loss: 2.12897\n",
            "training loss: 2.12895\n",
            "training loss: 2.12894\n",
            "training loss: 2.12892\n",
            "training loss: 2.12891\n",
            "training loss: 2.12889\n",
            "training loss: 2.12888\n",
            "training loss: 2.12887\n",
            "training loss: 2.12885\n",
            "training loss: 2.12884\n",
            "training loss: 2.12882\n",
            "training loss: 2.12881\n",
            "training loss: 2.12879\n",
            "training loss: 2.12878\n",
            "training loss: 2.12876\n",
            "training loss: 2.12875\n",
            "training loss: 2.12873\n",
            "training loss: 2.12872\n",
            "training loss: 2.12871\n",
            "training loss: 2.12869\n",
            "training loss: 2.12868\n",
            "training loss: 2.12866\n",
            "training loss: 2.12865\n",
            "training loss: 2.12864\n",
            "training loss: 2.12862\n",
            "training loss: 2.12861\n",
            "training loss: 2.12859\n",
            "training loss: 2.12858\n",
            "training loss: 2.12857\n",
            "training loss: 2.12855\n",
            "training loss: 2.12854\n",
            "training loss: 2.12853\n",
            "training loss: 2.12851\n",
            "training loss: 2.12850\n",
            "training loss: 2.12849\n",
            "training loss: 2.12847\n",
            "training loss: 2.12846\n",
            "training loss: 2.12845\n",
            "training loss: 2.12843\n",
            "training loss: 2.12842\n",
            "training loss: 2.12841\n",
            "training loss: 2.12839\n",
            "training loss: 2.12838\n",
            "training loss: 2.12837\n",
            "training loss: 2.12835\n",
            "training loss: 2.12834\n",
            "training loss: 2.12833\n",
            "training loss: 2.12832\n",
            "training loss: 2.12830\n",
            "training loss: 2.12829\n",
            "training loss: 2.12828\n",
            "training loss: 2.12827\n",
            "training loss: 2.12825\n",
            "training loss: 2.12824\n",
            "training loss: 2.12823\n",
            "training loss: 2.12822\n",
            "training loss: 2.12820\n",
            "training loss: 2.12819\n",
            "training loss: 2.12818\n",
            "training loss: 2.12817\n",
            "training loss: 2.12815\n",
            "training loss: 2.12814\n",
            "training loss: 2.12813\n",
            "training loss: 2.12812\n",
            "training loss: 2.12811\n",
            "training loss: 2.12809\n",
            "training loss: 2.12808\n",
            "training loss: 2.12807\n",
            "training loss: 2.12806\n",
            "training loss: 2.12805\n",
            "training loss: 2.12804\n",
            "training loss: 2.12802\n",
            "training loss: 2.12801\n",
            "training loss: 2.12800\n",
            "training loss: 2.12799\n",
            "training loss: 2.12798\n",
            "training loss: 2.12797\n",
            "training loss: 2.12795\n",
            "training loss: 2.12794\n",
            "training loss: 2.12793\n",
            "training loss: 2.12792\n",
            "training loss: 2.12791\n",
            "training loss: 2.12790\n",
            "training loss: 2.12789\n",
            "training loss: 2.12788\n",
            "training loss: 2.12786\n",
            "training loss: 2.12785\n",
            "training loss: 2.12784\n",
            "training loss: 2.12783\n",
            "training loss: 2.12782\n",
            "training loss: 2.12781\n",
            "training loss: 2.12780\n",
            "training loss: 2.12779\n",
            "training loss: 2.12778\n",
            "training loss: 2.12777\n",
            "training loss: 2.12776\n",
            "training loss: 2.12774\n",
            "training loss: 2.12773\n",
            "training loss: 2.12772\n",
            "training loss: 2.12771\n",
            "training loss: 2.12770\n",
            "training loss: 2.12769\n",
            "training loss: 2.12768\n",
            "training loss: 2.12767\n",
            "training loss: 2.12766\n",
            "training loss: 2.12765\n",
            "training loss: 2.12764\n",
            "training loss: 2.12763\n",
            "training loss: 2.12762\n",
            "training loss: 2.12761\n",
            "training loss: 2.12760\n",
            "training loss: 2.12759\n",
            "training loss: 2.12758\n",
            "training loss: 2.12757\n",
            "training loss: 2.12756\n",
            "training loss: 2.12755\n",
            "training loss: 2.12754\n",
            "training loss: 2.12753\n",
            "training loss: 2.12752\n",
            "training loss: 2.12751\n",
            "training loss: 2.12750\n",
            "training loss: 2.12749\n",
            "training loss: 2.12748\n",
            "training loss: 2.12747\n",
            "training loss: 2.12746\n",
            "training loss: 2.12745\n",
            "training loss: 2.12744\n",
            "training loss: 2.12743\n",
            "training loss: 2.12742\n",
            "training loss: 2.12741\n",
            "training loss: 2.12740\n",
            "training loss: 2.12739\n",
            "training loss: 2.12738\n",
            "training loss: 2.12737\n",
            "training loss: 2.12736\n",
            "training loss: 2.12735\n",
            "training loss: 2.12734\n",
            "training loss: 2.12734\n",
            "training loss: 2.12733\n",
            "training loss: 2.12732\n",
            "training loss: 2.12731\n",
            "training loss: 2.12730\n",
            "training loss: 2.12729\n",
            "training loss: 2.12728\n",
            "training loss: 2.12727\n",
            "training loss: 2.12726\n",
            "training loss: 2.12725\n",
            "training loss: 2.12724\n",
            "training loss: 2.12723\n",
            "training loss: 2.12723\n",
            "training loss: 2.12722\n",
            "training loss: 2.12721\n",
            "training loss: 2.12720\n",
            "training loss: 2.12719\n",
            "training loss: 2.12718\n",
            "training loss: 2.12717\n",
            "training loss: 2.12716\n",
            "training loss: 2.12716\n",
            "training loss: 2.12715\n",
            "training loss: 2.12714\n",
            "training loss: 2.12713\n",
            "training loss: 2.12712\n",
            "training loss: 2.12711\n",
            "training loss: 2.12710\n",
            "training loss: 2.12709\n",
            "training loss: 2.12709\n",
            "training loss: 2.12708\n",
            "training loss: 2.12707\n",
            "training loss: 2.12706\n",
            "training loss: 2.12705\n",
            "training loss: 2.12704\n",
            "training loss: 2.12704\n",
            "training loss: 2.12703\n",
            "training loss: 2.12702\n",
            "training loss: 2.12701\n",
            "training loss: 2.12700\n",
            "training loss: 2.12699\n",
            "training loss: 2.12699\n",
            "training loss: 2.12698\n",
            "training loss: 2.12697\n",
            "training loss: 2.12696\n",
            "training loss: 2.12695\n",
            "training loss: 2.12695\n",
            "training loss: 2.12694\n",
            "training loss: 2.12693\n",
            "training loss: 2.12692\n",
            "training loss: 2.12691\n",
            "training loss: 2.12691\n",
            "training loss: 2.12690\n",
            "training loss: 2.12689\n",
            "training loss: 2.12688\n",
            "training loss: 2.12687\n",
            "training loss: 2.12687\n",
            "training loss: 2.12686\n",
            "training loss: 2.12685\n",
            "training loss: 2.12684\n",
            "training loss: 2.12683\n",
            "training loss: 2.12683\n",
            "training loss: 2.12682\n",
            "training loss: 2.12681\n",
            "training loss: 2.12680\n",
            "training loss: 2.12680\n",
            "training loss: 2.12679\n",
            "training loss: 2.12678\n",
            "training loss: 2.12677\n",
            "training loss: 2.12677\n",
            "training loss: 2.12676\n",
            "training loss: 2.12675\n",
            "training loss: 2.12674\n",
            "training loss: 2.12674\n",
            "training loss: 2.12673\n",
            "training loss: 2.12672\n",
            "training loss: 2.12671\n",
            "training loss: 2.12671\n",
            "training loss: 2.12670\n",
            "training loss: 2.12669\n",
            "training loss: 2.12669\n",
            "training loss: 2.12668\n",
            "training loss: 2.12667\n",
            "training loss: 2.12666\n",
            "training loss: 2.12666\n",
            "training loss: 2.12665\n",
            "training loss: 2.12664\n",
            "training loss: 2.12663\n",
            "training loss: 2.12663\n",
            "training loss: 2.12662\n",
            "training loss: 2.12661\n",
            "training loss: 2.12661\n",
            "training loss: 2.12660\n",
            "training loss: 2.12659\n",
            "training loss: 2.12659\n",
            "training loss: 2.12658\n",
            "training loss: 2.12657\n",
            "training loss: 2.12656\n",
            "training loss: 2.12656\n",
            "training loss: 2.12655\n",
            "training loss: 2.12654\n",
            "training loss: 2.12654\n",
            "training loss: 2.12653\n",
            "training loss: 2.12652\n",
            "training loss: 2.12652\n",
            "training loss: 2.12651\n",
            "training loss: 2.12650\n",
            "training loss: 2.12650\n",
            "training loss: 2.12649\n",
            "training loss: 2.12648\n",
            "training loss: 2.12648\n",
            "training loss: 2.12647\n",
            "training loss: 2.12646\n",
            "training loss: 2.12646\n",
            "training loss: 2.12645\n",
            "training loss: 2.12644\n",
            "training loss: 2.12644\n",
            "training loss: 2.12643\n",
            "training loss: 2.12642\n",
            "training loss: 2.12642\n",
            "training loss: 2.12641\n",
            "training loss: 2.12641\n",
            "training loss: 2.12640\n",
            "training loss: 2.12639\n",
            "training loss: 2.12639\n",
            "training loss: 2.12638\n",
            "training loss: 2.12637\n",
            "training loss: 2.12637\n",
            "training loss: 2.12636\n",
            "training loss: 2.12635\n",
            "training loss: 2.12635\n",
            "training loss: 2.12634\n",
            "training loss: 2.12634\n",
            "training loss: 2.12633\n",
            "training loss: 2.12632\n",
            "training loss: 2.12632\n",
            "training loss: 2.12631\n",
            "training loss: 2.12630\n",
            "training loss: 2.12630\n",
            "training loss: 2.12629\n",
            "training loss: 2.12629\n",
            "training loss: 2.12628\n",
            "training loss: 2.12627\n",
            "training loss: 2.12627\n",
            "training loss: 2.12626\n",
            "training loss: 2.12626\n",
            "training loss: 2.12625\n",
            "training loss: 2.12624\n",
            "training loss: 2.12624\n",
            "training loss: 2.12623\n",
            "training loss: 2.12623\n",
            "training loss: 2.12622\n",
            "training loss: 2.12621\n",
            "training loss: 2.12621\n",
            "training loss: 2.12620\n",
            "training loss: 2.12620\n",
            "training loss: 2.12619\n",
            "training loss: 2.12619\n",
            "training loss: 2.12618\n",
            "training loss: 2.12617\n",
            "training loss: 2.12617\n",
            "training loss: 2.12616\n",
            "training loss: 2.12616\n",
            "training loss: 2.12615\n",
            "training loss: 2.12614\n",
            "training loss: 2.12614\n",
            "training loss: 2.12613\n",
            "training loss: 2.12613\n",
            "training loss: 2.12612\n",
            "training loss: 2.12612\n",
            "training loss: 2.12611\n",
            "training loss: 2.12611\n",
            "training loss: 2.12610\n",
            "training loss: 2.12609\n",
            "training loss: 2.12609\n",
            "training loss: 2.12608\n",
            "training loss: 2.12608\n",
            "training loss: 2.12607\n",
            "training loss: 2.12607\n",
            "training loss: 2.12606\n",
            "training loss: 2.12605\n",
            "training loss: 2.12604\n",
            "training loss: 2.12604\n",
            "training loss: 2.12603\n",
            "training loss: 2.12603\n",
            "training loss: 2.12602\n",
            "training loss: 2.12602\n",
            "training loss: 2.12601\n",
            "training loss: 2.12601\n",
            "training loss: 2.12600\n",
            "training loss: 2.12600\n",
            "training loss: 2.12599\n",
            "training loss: 2.12599\n",
            "training loss: 2.12598\n",
            "training loss: 2.12598\n",
            "training loss: 2.12597\n",
            "training loss: 2.12597\n",
            "training loss: 2.12596\n",
            "training loss: 2.12595\n",
            "training loss: 2.12595\n",
            "training loss: 2.12594\n",
            "training loss: 2.12594\n",
            "training loss: 2.12593\n",
            "training loss: 2.12593\n",
            "training loss: 2.12592\n",
            "training loss: 2.12592\n",
            "training loss: 2.12591\n",
            "training loss: 2.12591\n",
            "training loss: 2.12590\n",
            "training loss: 2.12590\n",
            "training loss: 2.12589\n",
            "training loss: 2.12589\n",
            "training loss: 2.12588\n",
            "training loss: 2.12588\n",
            "training loss: 2.12587\n",
            "training loss: 2.12587\n",
            "training loss: 2.12586\n",
            "training loss: 2.12586\n",
            "training loss: 2.12585\n",
            "training loss: 2.12585\n",
            "training loss: 2.12584\n",
            "training loss: 2.12584\n",
            "training loss: 2.12583\n",
            "training loss: 2.12583\n",
            "training loss: 2.12583\n",
            "training loss: 2.12582\n",
            "training loss: 2.12582\n",
            "training loss: 2.12581\n",
            "training loss: 2.12581\n",
            "training loss: 2.12580\n",
            "training loss: 2.12580\n",
            "training loss: 2.12579\n",
            "training loss: 2.12579\n",
            "training loss: 2.12578\n",
            "training loss: 2.12578\n",
            "training loss: 2.12577\n",
            "training loss: 2.12577\n",
            "training loss: 2.12576\n",
            "training loss: 2.12576\n",
            "training loss: 2.12575\n",
            "training loss: 2.12575\n",
            "training loss: 2.12574\n",
            "training loss: 2.12574\n",
            "training loss: 2.12574\n",
            "training loss: 2.12573\n",
            "training loss: 2.12573\n",
            "training loss: 2.12572\n",
            "training loss: 2.12572\n",
            "training loss: 2.12571\n",
            "training loss: 2.12571\n",
            "training loss: 2.12570\n",
            "training loss: 2.12570\n",
            "training loss: 2.12569\n",
            "training loss: 2.12569\n",
            "training loss: 2.12569\n",
            "training loss: 2.12568\n",
            "training loss: 2.12568\n",
            "training loss: 2.12567\n",
            "training loss: 2.12567\n",
            "training loss: 2.12566\n",
            "training loss: 2.12566\n",
            "training loss: 2.12565\n",
            "training loss: 2.12565\n",
            "training loss: 2.12565\n",
            "training loss: 2.12564\n",
            "training loss: 2.12564\n",
            "training loss: 2.12563\n",
            "training loss: 2.12563\n",
            "training loss: 2.12562\n",
            "training loss: 2.12562\n",
            "training loss: 2.12562\n",
            "training loss: 2.12561\n",
            "training loss: 2.12561\n",
            "training loss: 2.12560\n",
            "training loss: 2.12560\n",
            "training loss: 2.12559\n",
            "training loss: 2.12559\n",
            "training loss: 2.12559\n",
            "training loss: 2.12558\n",
            "training loss: 2.12558\n",
            "training loss: 2.12557\n",
            "training loss: 2.12557\n",
            "training loss: 2.12557\n",
            "training loss: 2.12556\n",
            "training loss: 2.12556\n",
            "training loss: 2.12555\n",
            "training loss: 2.12555\n",
            "training loss: 2.12554\n",
            "training loss: 2.12554\n",
            "training loss: 2.12554\n",
            "training loss: 2.12553\n",
            "training loss: 2.12553\n",
            "training loss: 2.12552\n",
            "training loss: 2.12552\n",
            "training loss: 2.12552\n",
            "training loss: 2.12551\n",
            "training loss: 2.12551\n",
            "training loss: 2.12550\n",
            "training loss: 2.12550\n",
            "training loss: 2.12550\n",
            "training loss: 2.12549\n",
            "training loss: 2.12549\n",
            "training loss: 2.12548\n",
            "training loss: 2.12548\n",
            "training loss: 2.12548\n",
            "training loss: 2.12547\n",
            "training loss: 2.12547\n",
            "training loss: 2.12546\n",
            "training loss: 2.12546\n",
            "training loss: 2.12546\n",
            "training loss: 2.12545\n",
            "training loss: 2.12545\n",
            "training loss: 2.12545\n",
            "training loss: 2.12544\n",
            "training loss: 2.12544\n",
            "training loss: 2.12543\n",
            "training loss: 2.12543\n",
            "training loss: 2.12543\n",
            "training loss: 2.12542\n",
            "training loss: 2.12542\n",
            "training loss: 2.12541\n",
            "training loss: 2.12541\n",
            "training loss: 2.12541\n",
            "training loss: 2.12540\n",
            "training loss: 2.12540\n",
            "training loss: 2.12540\n",
            "training loss: 2.12539\n",
            "training loss: 2.12539\n",
            "training loss: 2.12538\n",
            "training loss: 2.12538\n",
            "training loss: 2.12538\n",
            "training loss: 2.12537\n",
            "training loss: 2.12537\n",
            "training loss: 2.12537\n",
            "training loss: 2.12536\n",
            "training loss: 2.12536\n",
            "training loss: 2.12536\n",
            "training loss: 2.12535\n",
            "training loss: 2.12535\n",
            "training loss: 2.12534\n",
            "training loss: 2.12534\n",
            "training loss: 2.12534\n",
            "training loss: 2.12533\n",
            "training loss: 2.12533\n",
            "training loss: 2.12533\n",
            "training loss: 2.12532\n",
            "training loss: 2.12532\n",
            "training loss: 2.12532\n",
            "training loss: 2.12531\n",
            "training loss: 2.12531\n",
            "training loss: 2.12530\n",
            "training loss: 2.12530\n",
            "training loss: 2.12530\n",
            "training loss: 2.12529\n",
            "training loss: 2.12529\n",
            "training loss: 2.12529\n",
            "training loss: 2.12528\n",
            "training loss: 2.12528\n",
            "training loss: 2.12528\n",
            "training loss: 2.12527\n",
            "training loss: 2.12527\n",
            "training loss: 2.12527\n",
            "training loss: 2.12526\n",
            "training loss: 2.12526\n",
            "training loss: 2.12526\n",
            "training loss: 2.12525\n",
            "training loss: 2.12525\n",
            "training loss: 2.12525\n",
            "training loss: 2.12524\n",
            "training loss: 2.12524\n",
            "training loss: 2.12524\n",
            "training loss: 2.12523\n",
            "training loss: 2.12523\n",
            "training loss: 2.12523\n",
            "training loss: 2.12522\n",
            "training loss: 2.12522\n",
            "training loss: 2.12522\n",
            "training loss: 2.12521\n",
            "training loss: 2.12521\n",
            "training loss: 2.12521\n",
            "training loss: 2.12520\n",
            "training loss: 2.12520\n",
            "training loss: 2.12520\n",
            "training loss: 2.12519\n",
            "training loss: 2.12519\n",
            "training loss: 2.12519\n",
            "training loss: 2.12518\n",
            "training loss: 2.12518\n",
            "training loss: 2.12518\n",
            "training loss: 2.12517\n",
            "training loss: 2.12517\n",
            "training loss: 2.12517\n",
            "training loss: 2.12516\n",
            "training loss: 2.12516\n",
            "training loss: 2.12516\n",
            "training loss: 2.12515\n",
            "training loss: 2.12515\n",
            "training loss: 2.12515\n",
            "training loss: 2.12514\n",
            "training loss: 2.12514\n",
            "training loss: 2.12514\n",
            "training loss: 2.12514\n",
            "training loss: 2.12513\n",
            "training loss: 2.12513\n",
            "training loss: 2.12513\n",
            "training loss: 2.12512\n",
            "training loss: 2.12512\n",
            "training loss: 2.12512\n",
            "training loss: 2.12511\n",
            "training loss: 2.12511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "# Create a color map for the segmentation labels\n",
        "colors = np.array([ (0, 0, 0),  # background\n",
        "               (128, 0, 0), # aeroplane\n",
        "               (0, 128, 0), # bicycle\n",
        "               (128, 128, 0), # bird\n",
        "               (0, 0, 128), # boat\n",
        "               (128, 0, 128), # bottle\n",
        "               (0, 128, 128), # bus \n",
        "               (128, 128, 128), # car\n",
        "               (64, 0, 0), # cat\n",
        "               (192, 0, 0), # chair\n",
        "               (64, 128, 0), # cow\n",
        "               (192, 128, 0), # dining table\n",
        "               (64, 0, 128), # dog\n",
        "               (192, 0, 128), # horse\n",
        "               (64, 128, 128), # motorbike\n",
        "               (192, 128, 128), # person\n",
        "               (0, 64, 0), # potted plant\n",
        "               (128, 64, 0), # sheep\n",
        "               (0, 192, 0), # sofa\n",
        "               (128, 192, 0), # train\n",
        "               (0, 64, 128)]) #\n",
        "\n"
      ],
      "metadata": {
        "id": "ADwMsXX4iUpB"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def metric_calculation(image_path, mask_path, model):\n",
        "  # Load and preprocess the image\n",
        "  img = cv2.imread(image_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  img_tensor = transforms.ToTensor()(img)\n",
        "  img_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img_tensor)\n",
        "  img_tensor = img_tensor.unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "  # Make the prediction\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      output = model(img_tensor)\n",
        "      \n",
        "  output = output.argmax(1).squeeze().detach().cpu().numpy()\n",
        "\n",
        "  colored_output = colors[output]\n",
        "\n",
        "  output = colored_output\n",
        "\n",
        "  # Load the ground truth label\n",
        "  label = cv2.imread(mask_path)\n",
        "  label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
        "  label = np.resize(label, output.shape)\n",
        "\n",
        "  # Calculate pixel accuracy\n",
        "  pixel_acc = (output == label).astype(int).sum() / label.size\n",
        "\n",
        "\n",
        "  # Calculate mIoU\n",
        "  iou = []\n",
        "  for class_id in colors:\n",
        "      true_positives = ((output == class_id) & (label == class_id)).sum()\n",
        "      false_positives = ((output == class_id) & (label != class_id)).sum()\n",
        "      false_negatives = ((output != class_id) & (label == class_id)).sum()\n",
        "\n",
        "      if true_positives + false_positives + false_negatives == 0:\n",
        "          iou.append(0)\n",
        "      else:\n",
        "          iou.append(true_positives / (true_positives + false_positives + false_negatives))\n",
        "\n",
        "  miou = np.mean(iou)\n",
        "\n",
        "  return pixel_acc, miou\n"
      ],
      "metadata": {
        "id": "gU247osE7OCD"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/PascalVOC/test/Images/000068.jpg\"\n",
        "mask_path = \"/content/PascalVOC/test/Annotations/000068.png\"\n",
        "print(metric_calculation(image_path, mask_path, model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQim70XeXMjh",
        "outputId": "eeffd3f4-d480-4f3e-e161-4d01d29e263c"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.9216274685329862, 0.7682738157040111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_name = []\n",
        "for a, b, c in os.walk(\"/content/PascalVOC/test/Annotations\"):\n",
        "  annotation_name = c\n",
        "anno = []\n",
        "for string in annotation_name:\n",
        "  anno1 = string.replace(\".png\", \"\")\n",
        "  anno.append(anno1)"
      ],
      "metadata": {
        "id": "0SylwWEbXgA5"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_acc_cumulative = 0\n",
        "mIoU_cumulative = 0\n",
        "for image_name in tqdm(anno):\n",
        "  a, b = metric_calculation(f\"/content/PascalVOC/test/Images/{image_name}.jpg\",f\"/content/PascalVOC/test/Annotations/{image_name}.png\", model)\n",
        "  pixel_acc_cumulative = pixel_acc_cumulative + a\n",
        "  mIoU_cumulative = mIoU_cumulative + b\n",
        "print(f\"Average Pixel Accuracy over Test set: {pixel_acc_cumulative/len(anno):.5f}\")\n",
        "print(f\"Average mIoU over Test set: {mIoU_cumulative/len(anno):.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "11f6876814e8448a925acc35a49cc398",
            "7c6721184ab24177bcc0167606977e5d",
            "cd6a309d6eac473889a08854e5561146",
            "7734f238e22146ad87ba2c02bc7d7eab",
            "940841a88e964351b628dfd63e21cc35",
            "af87c846731846cb8f12a7ed82b1c4b1",
            "69c33ce7c0a44bf09eea20bb64f4b89a",
            "c4329077db794c7b84e873b6e758bdb8",
            "812916fe7d8f4b768a6303185b1faaa7",
            "e802410d665640a8b1aef4029dd4c24c",
            "98b1679f9a5b4a7aa2cd1a6b6e5486b1"
          ]
        },
        "id": "XIXlF77JYd1m",
        "outputId": "8662b8bf-7493-4a28-8d0f-2af0f9e7d443"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/210 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11f6876814e8448a925acc35a49cc398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Pixel Accuracy over Test set: 0.76564\n",
            "Average mIoU over Test set: 0.61378\n"
          ]
        }
      ]
    }
  ]
}